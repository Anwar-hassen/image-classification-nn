{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a99cad-09a4-46bd-b39f-f882ce3e44b0",
   "metadata": {},
   "source": [
    "# Neural Networks \n",
    "- This project will I wil be using neural networks by implementing a multilayer perceptron (MLP) model in PyTorch to classify handwritten digits from the Digits dataset.\n",
    "- The dataset contains 1,797 images of handwritten digits, each image being an 8x8 pixel grayscale\n",
    "image of a digit (0-9). Each image is represented as a 64-feature input vector, corresponding to\n",
    "the grayscale values of the pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd233b7f-032d-422f-afa2-0efb3659580d",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3a2331-f69f-4d28-873f-4aa6cb0305e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\anwar\\anaconda3\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\anwar\\anaconda3\\lib\\site-packages (0.22.0+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\anwar\\anaconda3\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\anwar\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\anwar\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\anwar\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\anwar\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anwar\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\anwar\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\anwar\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\anwar\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\anwar\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\anwar\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anwar\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "import torch  # Pytorch is imported as torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cbda81a-3468-4d14-9cc8-8ef85141e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f78ab-6ca2-4137-bbde-b52402a506eb",
   "metadata": {},
   "source": [
    "### Load & Scale Digits Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "775de7d8-67c3-4126-9590-f361318c716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(231)\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data  \n",
    "y = digits.target  \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed941f-07a7-4551-b1c9-00c658b4d2c3",
   "metadata": {},
   "source": [
    "### Prepare Data for PyTorch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "294fbec3-4556-4399-a496-a1dd7fad0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=231)\n",
    "\n",
    "# Converting NumPy arrays to PyTorch tensors for model training\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Creating dataset objects for PyTorch\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Use DataLoader to load data in batches for efficient training\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c038b515-b2a8-4481-946f-9ce2f5764d75",
   "metadata": {},
   "source": [
    "### Define Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "084c1da9-4324-4d8c-ad39-ed1d0bcea547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 128)  \n",
    "        self.fc2 = nn.Linear(128, 10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x  \n",
    "model = DigitClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97085b11-c13e-445e-9188-b665c152c4ae",
   "metadata": {},
   "source": [
    "### Defining Loss Function, Optimizer, and Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89e26ae4-e399-43a6-85f4-cb149255ae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 2.072, Accuracy = 58.1%\n",
      "Epoch 2: Loss = 1.403, Accuracy = 87.0%\n",
      "Epoch 3: Loss = 0.803, Accuracy = 91.2%\n",
      "Epoch 4: Loss = 0.511, Accuracy = 92.1%\n",
      "Epoch 5: Loss = 0.374, Accuracy = 93.5%\n",
      "Epoch 6: Loss = 0.298, Accuracy = 94.5%\n",
      "Epoch 7: Loss = 0.249, Accuracy = 95.2%\n",
      "Epoch 8: Loss = 0.212, Accuracy = 96.0%\n",
      "Epoch 9: Loss = 0.185, Accuracy = 96.5%\n",
      "Epoch 10: Loss = 0.167, Accuracy = 96.8%\n",
      "Epoch 11: Loss = 0.151, Accuracy = 97.1%\n",
      "Epoch 12: Loss = 0.139, Accuracy = 97.5%\n",
      "Epoch 13: Loss = 0.127, Accuracy = 98.1%\n",
      "Epoch 14: Loss = 0.119, Accuracy = 97.6%\n",
      "Epoch 15: Loss = 0.112, Accuracy = 98.1%\n"
     ]
    }
   ],
   "source": [
    "# Defining the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # using Adam for faster convergence\n",
    "\n",
    "# using a function to train the neural network\n",
    "def train(model, loader, criterion, optimizer, epochs=15):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_X, batch_y in loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_X.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        # Computing the average loss and accuracy for the epoch\n",
    "        avg_loss = running_loss / total\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.3f}, Accuracy = {accuracy:.1f}%\")\n",
    "# Training the model using the DataLoader\n",
    "train(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3ea9b-5fdd-4562-8d51-8129b19b776e",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance on Test Set with Accuracy and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc6d8b6c-c276-4715-9489-ecdde10e5457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 95.3%\n",
      "Sample Predictions:\n",
      "Image 1: Predicted = 4, Actual = 4\n",
      "Image 2: Predicted = 5, Actual = 5\n",
      "Image 3: Predicted = 1, Actual = 1\n",
      "Image 4: Predicted = 4, Actual = 4\n",
      "Image 5: Predicted = 1, Actual = 1\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model on the test dataset\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    examples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "            # Collect 5 example predictions for display\n",
    "            for i in range(len(batch_y)):\n",
    "                if len(examples) < 5: \n",
    "                    examples.append((batch_X[i], predicted[i].item(), batch_y[i].item()))\n",
    "                else:\n",
    "                    break\n",
    "     # Compute accuracy percentage\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.1f}%\")\n",
    "    print(\"Sample Predictions:\")\n",
    "    for i, (img, pred, actual) in enumerate(examples):\n",
    "        print(f\"Image {i+1}: Predicted = {pred}, Actual = {actual}\")\n",
    "\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09348e9c-f2fa-4c05-903a-0302eb279457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AltActivationNet(\n",
       "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neural network that allows different activation functions\n",
    "\n",
    "class AltActivationNet(nn.Module):\n",
    "    def __init__(self, activation='sigmoid'):\n",
    "        super(AltActivationNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            x = torch.sigmoid(self.fc1(x))\n",
    "        elif self.activation == 'tanh':\n",
    "            x = torch.tanh(self.fc1(x))\n",
    "        else:\n",
    "            x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "AltActivationNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6aaa2a3b-986f-4e11-9ccc-b185663e168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(activation):\n",
    "    model = AltActivationNet(activation=activation)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"\\nTraining with activation: {activation}\")\n",
    "    train(model, train_loader, criterion, optimizer)\n",
    "    evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2b925f4-3b89-4e48-ab9e-d05242a11fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with activation: sigmoid\n",
      "Epoch 1: Loss = 2.270, Accuracy = 24.5%\n",
      "Epoch 2: Loss = 2.137, Accuracy = 49.4%\n",
      "Epoch 3: Loss = 1.985, Accuracy = 65.9%\n",
      "Epoch 4: Loss = 1.788, Accuracy = 77.3%\n",
      "Epoch 5: Loss = 1.558, Accuracy = 83.6%\n",
      "Epoch 6: Loss = 1.329, Accuracy = 83.1%\n",
      "Epoch 7: Loss = 1.126, Accuracy = 87.9%\n",
      "Epoch 8: Loss = 0.956, Accuracy = 87.6%\n",
      "Epoch 9: Loss = 0.822, Accuracy = 88.8%\n",
      "Epoch 10: Loss = 0.714, Accuracy = 90.2%\n",
      "Epoch 11: Loss = 0.628, Accuracy = 91.2%\n",
      "Epoch 12: Loss = 0.559, Accuracy = 91.0%\n",
      "Epoch 13: Loss = 0.501, Accuracy = 92.3%\n",
      "Epoch 14: Loss = 0.456, Accuracy = 92.7%\n",
      "Epoch 15: Loss = 0.413, Accuracy = 92.8%\n",
      "\n",
      "Test Accuracy: 93.1%\n",
      "Sample Predictions:\n",
      "Image 1: Predicted = 4, Actual = 4\n",
      "Image 2: Predicted = 5, Actual = 5\n",
      "Image 3: Predicted = 1, Actual = 1\n",
      "Image 4: Predicted = 4, Actual = 4\n",
      "Image 5: Predicted = 2, Actual = 1\n",
      "\n",
      "Training with activation: tanh\n",
      "Epoch 1: Loss = 1.984, Accuracy = 55.6%\n",
      "Epoch 2: Loss = 1.274, Accuracy = 86.3%\n",
      "Epoch 3: Loss = 0.749, Accuracy = 89.2%\n",
      "Epoch 4: Loss = 0.495, Accuracy = 91.9%\n",
      "Epoch 5: Loss = 0.364, Accuracy = 93.2%\n",
      "Epoch 6: Loss = 0.287, Accuracy = 94.4%\n",
      "Epoch 7: Loss = 0.239, Accuracy = 94.9%\n",
      "Epoch 8: Loss = 0.204, Accuracy = 96.0%\n",
      "Epoch 9: Loss = 0.180, Accuracy = 96.0%\n",
      "Epoch 10: Loss = 0.162, Accuracy = 96.9%\n",
      "Epoch 11: Loss = 0.144, Accuracy = 97.0%\n",
      "Epoch 12: Loss = 0.133, Accuracy = 97.2%\n",
      "Epoch 13: Loss = 0.125, Accuracy = 97.4%\n",
      "Epoch 14: Loss = 0.116, Accuracy = 97.9%\n",
      "Epoch 15: Loss = 0.106, Accuracy = 98.1%\n",
      "\n",
      "Test Accuracy: 94.4%\n",
      "Sample Predictions:\n",
      "Image 1: Predicted = 4, Actual = 4\n",
      "Image 2: Predicted = 5, Actual = 5\n",
      "Image 3: Predicted = 1, Actual = 1\n",
      "Image 4: Predicted = 4, Actual = 4\n",
      "Image 5: Predicted = 1, Actual = 1\n",
      "\n",
      "Training with activation: relu\n",
      "Epoch 1: Loss = 2.089, Accuracy = 55.3%\n",
      "Epoch 2: Loss = 1.431, Accuracy = 82.0%\n",
      "Epoch 3: Loss = 0.830, Accuracy = 88.1%\n",
      "Epoch 4: Loss = 0.528, Accuracy = 91.9%\n",
      "Epoch 5: Loss = 0.386, Accuracy = 93.2%\n",
      "Epoch 6: Loss = 0.303, Accuracy = 94.6%\n",
      "Epoch 7: Loss = 0.255, Accuracy = 95.5%\n",
      "Epoch 8: Loss = 0.219, Accuracy = 95.5%\n",
      "Epoch 9: Loss = 0.189, Accuracy = 96.0%\n",
      "Epoch 10: Loss = 0.167, Accuracy = 96.7%\n",
      "Epoch 11: Loss = 0.153, Accuracy = 97.1%\n",
      "Epoch 12: Loss = 0.142, Accuracy = 97.1%\n",
      "Epoch 13: Loss = 0.127, Accuracy = 97.7%\n",
      "Epoch 14: Loss = 0.119, Accuracy = 97.8%\n",
      "Epoch 15: Loss = 0.109, Accuracy = 98.0%\n",
      "\n",
      "Test Accuracy: 95.8%\n",
      "Sample Predictions:\n",
      "Image 1: Predicted = 4, Actual = 4\n",
      "Image 2: Predicted = 5, Actual = 5\n",
      "Image 3: Predicted = 1, Actual = 1\n",
      "Image 4: Predicted = 4, Actual = 4\n",
      "Image 5: Predicted = 1, Actual = 1\n"
     ]
    }
   ],
   "source": [
    "# make loop\n",
    "for act in ['sigmoid', 'tanh', 'relu']:\n",
    "    train_and_test_model(act)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af731720-f4bb-4d10-8284-8c29bbbac4c4",
   "metadata": {},
   "source": [
    "#### ReLU achieved the highest test accuracy at 95.8%, outperforming tanh (94.4%) and sigmoid (93.1%). Tanh and ReLU both reached ~98% training accuracy, while sigmoid lagged behind at ~92.8%. This was expected as ReLU is better suited for better suited for neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
